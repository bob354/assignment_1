{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bd138940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4e70d",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "655c6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a89f38",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e0b421e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "x_train_temp, x_test, y_train_temp, y_test = train_test_split(x, y, test_size=0.2, random_state=67)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_temp, y_train_temp, test_size=0.25, random_state=67)\n",
    "# print(x_valid.shape)\n",
    "# print(y_valid.shape)\n",
    "# plt.gray()\n",
    "# plt.matshow(digits.images[67])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3cc8e9",
   "metadata": {},
   "source": [
    "# Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "90147e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a0de6",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "790d8a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "penalty_types = ['l1', 'l2']\n",
    "best_paramsLR = {}\n",
    "best_modelLR = None\n",
    "best_accLR = 0\n",
    "\n",
    "for c in C_values:\n",
    "    for p in penalty_types:\n",
    "        cur_modelLR = LogisticRegression(C=c, penalty=p, solver='liblinear', max_iter=1000, random_state=67)\n",
    "        cur_modelLR.fit(x_train, y_train)\n",
    "        cur_acc = cur_modelLR.score(x_valid, y_valid)\n",
    "\n",
    "        if cur_acc > best_accLR:\n",
    "            best_accLR = cur_acc\n",
    "            best_modelLR = cur_modelLR\n",
    "            best_paramsLR = {'penalty types': p, \"C value\": c}\n",
    "\n",
    "y_pred_LR = best_modelLR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1c797a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.98      0.87      0.92        46\n",
      "           2       0.97      1.00      0.98        31\n",
      "           3       0.92      0.95      0.93        37\n",
      "           4       0.98      0.98      0.98        42\n",
      "           5       0.97      0.89      0.93        36\n",
      "           6       0.89      1.00      0.94        33\n",
      "           7       0.91      1.00      0.95        31\n",
      "           8       0.80      0.83      0.81        29\n",
      "           9       0.93      0.88      0.90        42\n",
      "\n",
      "    accuracy                           0.94       360\n",
      "   macro avg       0.93      0.94      0.94       360\n",
      "weighted avg       0.94      0.94      0.94       360\n",
      "\n",
      "{'penalty types': 'l1', 'C value': 10}\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=y_pred_LR, y_true=y_test))\n",
    "print(best_paramsLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34917cbd",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bc51360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree\n",
    "max_depth_value = [5, 10, 15, None]\n",
    "min_samples_leaf_value = [1, 5, 10]\n",
    "best_paramsDT = {}\n",
    "best_modelDT = None\n",
    "best_accDT = 0\n",
    "\n",
    "for d in max_depth_value:\n",
    "    for l in min_samples_leaf_value:\n",
    "        cur_modelDT = DecisionTreeClassifier(max_depth=d, min_samples_leaf=l, random_state=67)\n",
    "        cur_modelDT.fit(x_train, y_train)\n",
    "        cur_acc = cur_modelDT.score(x_valid, y_valid)\n",
    "\n",
    "        if cur_acc > best_accDT:\n",
    "            best_accDT = cur_acc\n",
    "            best_modelDT = cur_modelDT\n",
    "            best_paramsDT = {'max depth': d, 'min samples leaf': l}\n",
    "\n",
    "y_pred_DT = best_modelDT.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "063cc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        33\n",
      "           1       0.88      0.80      0.84        46\n",
      "           2       0.88      0.71      0.79        31\n",
      "           3       0.74      0.76      0.75        37\n",
      "           4       0.95      0.93      0.94        42\n",
      "           5       0.91      0.81      0.85        36\n",
      "           6       0.80      0.97      0.88        33\n",
      "           7       0.76      0.94      0.84        31\n",
      "           8       0.62      0.69      0.66        29\n",
      "           9       0.74      0.69      0.72        42\n",
      "\n",
      "    accuracy                           0.82       360\n",
      "   macro avg       0.83      0.83      0.82       360\n",
      "weighted avg       0.83      0.82      0.82       360\n",
      "\n",
      "{'max depth': 15, 'min samples leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=y_pred_DT, y_true=y_test))\n",
    "print(best_paramsDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb43f3",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4379d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "k_values = [1, 3, 5, 7, 9, 11]\n",
    "weight_types = ['uniform', 'distance']\n",
    "metric_types = ['euclidean', 'manhattan'] \n",
    "best_modelKNN = None\n",
    "best_accKNN = 0\n",
    "best_paramsKNN = {}\n",
    "\n",
    "for k in k_values:\n",
    "    for w in weight_types:\n",
    "        for m in metric_types:\n",
    "            cur_modelKNN = KNeighborsClassifier(n_neighbors=k, weights=w, metric=m)\n",
    "            cur_modelKNN.fit(x_train, y_train)\n",
    "            cur_acc = cur_modelKNN.score(x_valid, y_valid)\n",
    "\n",
    "            if cur_acc > best_accKNN:\n",
    "                best_accKNN = cur_acc\n",
    "                best_modelKNN = cur_modelKNN\n",
    "                best_paramsKNN = {'k value': k, 'weight type': w, 'metric type': m}\n",
    "\n",
    "y_pred_KNN = best_modelKNN.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "641d001b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.98      0.98      0.98        46\n",
      "           2       1.00      1.00      1.00        31\n",
      "           3       0.95      0.97      0.96        37\n",
      "           4       0.98      1.00      0.99        42\n",
      "           5       0.97      0.94      0.96        36\n",
      "           6       0.97      1.00      0.99        33\n",
      "           7       0.97      1.00      0.98        31\n",
      "           8       0.96      0.93      0.95        29\n",
      "           9       0.97      0.93      0.95        42\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.97      0.97       360\n",
      "\n",
      "{'k value': 3, 'weight type': 'uniform', 'metric type': 'euclidean'}\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=y_pred_KNN, y_true=y_test))\n",
    "print(best_paramsKNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7df6f4",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4c5a34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_options = [(50,), (100,), (50, 50), (100, 100, 50)]\n",
    "alpha_values = [0.0001, 0.001, 0.01]\n",
    "learning_rates = [0.001, 0.0001]\n",
    "best_modelANN = None\n",
    "best_accANN = 0\n",
    "best_paramsANN = {}\n",
    "\n",
    "for h in hidden_layer_options:\n",
    "    for a in alpha_values:\n",
    "        for l in learning_rates:\n",
    "            cur_modelANN = MLPClassifier(hidden_layer_sizes=h, alpha=a, learning_rate_init=l, solver='adam', max_iter=1000, random_state=67)\n",
    "            cur_modelANN.fit(x_train, y_train)\n",
    "            cur_acc = cur_modelANN.score(x_valid, y_valid)\n",
    "            if best_accANN < cur_acc:\n",
    "                best_accANN = cur_acc\n",
    "                best_modelANN = cur_modelANN\n",
    "                best_paramsANN = {'hidden layer': h, 'alpha': a, 'learning rate': l}\n",
    "        \n",
    "y_pred_ANN = best_modelANN.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8a828250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.98      0.98      0.98        46\n",
      "           2       0.97      1.00      0.98        31\n",
      "           3       1.00      0.97      0.99        37\n",
      "           4       1.00      0.98      0.99        42\n",
      "           5       0.94      0.92      0.93        36\n",
      "           6       0.92      1.00      0.96        33\n",
      "           7       1.00      1.00      1.00        31\n",
      "           8       0.89      0.83      0.86        29\n",
      "           9       0.98      1.00      0.99        42\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "{'hidden layer': (100,), 'alpha': 0.0001, 'learning rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=y_pred_ANN, y_true=y_test))\n",
    "print(best_paramsANN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
